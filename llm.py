# llm.py
# This file handles communication with the Groq LLM API
# It is responsible ONLY for text generation using an LLM

# Import os to read environment variables from the system
import os

# Import Groq client to communicate with Groq-hosted LLM models
from groq import Groq

# Import load_dotenv to load variables from the .env file
from dotenv import load_dotenv

# Load environment variables from the .env file into the system
# This allows us to access GROQ_API_KEY using os.getenv()
load_dotenv()

def generate_text(prompt: str) -> str:
    """
    This function:
    - Accepts a prompt as input
    - Sends the prompt to the Groq LLM
    - Returns the generated text response
    """

    # Read the Groq API key from environment variables
    # This keeps the API key secure and out of source code
    api_key = os.getenv("GROQ_API_KEY")

    # If the API key is missing, raise an error immediately
    # This prevents silent failures during LLM calls
    if not api_key:
        raise ValueError("GROQ_API_KEY is missing in .env file")

    # Initialize the Groq client using the API key
    # This client is used to communicate with Groq servers
    client = Groq(api_key=api_key)

    # Call the Groq Chat Completion API
    # Even though this is a chat endpoint, it is used here for text generation
    response = client.chat.completions.create(
        model="llama-3.1-8b-instant",  # Lightweight, fast Groq-supported LLM
        messages=[
            # The user role contains the actual prompt we want the model to answer
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,   # Controls creativity (higher = more creative)
        max_tokens=200     # Limits the length of the generated response
    )

    # Extract the generated text from the response object
    # choices[0] → first generated completion
    # message.content → actual text generated by the LLM
    return response.choices[0].message.content